{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data from ARIMA models\n",
    "Running this notebook will:\n",
    "\n",
    "- create 9 ARIMA models\n",
    "- show and save plots of some data generated from these models\n",
    "- generate more X samples and create a dataframe with true labels \n",
    "- save used models and generated data to pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from ARIMA import *\n",
    "from sacf import *\n",
    "\n",
    "DATA_DIR = \"./generated_data\"\n",
    "FILENAME_SUFFIX = \"ARIMA_data_ntrain5\"\n",
    "N_TRAIN_SAMPLES_PER_MODEL = 5\n",
    "N_TEST_SAMPLES_PER_MODEL = 30\n",
    "MAX_SAMPLE_LEN = 50\n",
    "MIN_SAMPLE_LEN = 20\n",
    "\n",
    "time_index = int(time.time())\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARIMA_model:\n",
    "    \"\"\"Autoregressive integrated moving average model.\n",
    "\n",
    "    https://github.com/TOMILO87/time_series_simulation/blob/master/ARIMA\n",
    "    \"\"\"\n",
    "    def __init__(self, phi, theta, d = 0, t = 0, mu = 0, sigma = 1, burn = 10):\n",
    "        self.phi = phi\n",
    "        self.theta = theta\n",
    "        self.d = d\n",
    "        self.t = t\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.burn = burn\n",
    "\n",
    "    def sample(self, n):\n",
    "        return ARIMA(self.phi, self.theta, self.d, self.t, self.mu, self.sigma, n, self.burn)\n",
    "\n",
    "    @property\n",
    "    def p(self):\n",
    "        return len(self.phi)\n",
    "\n",
    "    @property\n",
    "    def q(self):\n",
    "        return len(self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stationary, difference in mean (t) or in variance (theta)\n",
    "model1 = ARIMA_model(np.array([0.8]), np.array([0.0]), t = 1)\n",
    "model2 = ARIMA_model(np.array([0.8]), np.array([2.0]), t = 1)\n",
    "model3 = ARIMA_model(np.array([0.8]), np.array([2.0]), t = 3)\n",
    "# Divergent, difference in mean (t) or in variance (theta)\n",
    "model4 = ARIMA_model(np.array([-1.01]), np.array([0.0]), t = 1)\n",
    "model5 = ARIMA_model(np.array([-1.01]), np.array([4.0]), t = 1)\n",
    "model6 = ARIMA_model(np.array([-1.01]), np.array([4.0]), t = 0)\n",
    "# ARIMA (d > 0)\n",
    "model7 = ARIMA_model(np.array([0.8]), np.array([0.0]), d = 1, t = 1)\n",
    "model8 = ARIMA_model(np.array([0.8]), np.array([2.0]), d = 1, t = 1)\n",
    "model9 = ARIMA_model(np.array([0.8]), np.array([2.0]), d = 1, t = 0)\n",
    "\n",
    "\n",
    "models_lst = [model1, model2, model3, model4, model5, model6, model7, model8, model9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "plt.rcParams[\"figure.figsize\"] = (40, 16)\n",
    "plt.rcParams['font.size'] = 25\n",
    "fig, axes = plt.subplots(3,3, sharey=\"all\")\n",
    "plt.suptitle(f\"Samples of length {n} (ARIMA)\")\n",
    "for i, ax in enumerate(axes.reshape(-1)):\n",
    "    m = models_lst[i]\n",
    "    try:\n",
    "        X = m.sample(n)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in model {i}: {e}\")\n",
    "        continue\n",
    "        \n",
    "    ax.plot(X, color = \"black\")\n",
    "    ax.set_title(f\"[{i}]: ARIMA(p= {m.p}, d= {m.d}, q= {m.q}, t = {m.t}, mu = {m.mu}, sigma = {m.sigma})\\nphi: {m.phi}; theta: {m.theta}\")\n",
    "    ax.grid()\n",
    "\n",
    "fig.tight_layout()   \n",
    "plt.savefig(f\"./plots/models_ARIMA_{time_index}.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(models_lst: list[ARIMA_model], samples_per_model: int, max_sample_len: int, min_sample_len: int) -> tuple:\n",
    "    \"\"\"Generate data from list of hmm.BaseHMM instances.\n",
    "    Returns:\n",
    "        - labels_df: pd.DataFrame | sample_id | true_label | sample_len\n",
    "        - all_X_samples: a list with generated X samples\n",
    "    \"\"\"\n",
    "    all_X_samples = []\n",
    "    true_labels = []\n",
    "    sample_lengths = []\n",
    "    sample_ids = list(range(samples_per_model*len(models_lst)))\n",
    "    len_step = (max_sample_len-min_sample_len)//10\n",
    "    possible_lenghts = range(min_sample_len, max_sample_len + 1, len_step)\n",
    "    \n",
    "    for i in range(len(models_lst)):\n",
    "        model = models_lst[i]\n",
    "        for j in range(samples_per_model):\n",
    "            sample_len = np.random.choice(possible_lenghts)\n",
    "            X = model.sample(sample_len)\n",
    "            all_X_samples.append(X)\n",
    "            sample_lengths.append(sample_len)\n",
    "            true_labels.append(i)\n",
    "\n",
    "    labels_df = pd.DataFrame({\n",
    "        \"sample_id\": sample_ids,\n",
    "        \"true_label\": true_labels,\n",
    "        \"sample_len\": sample_lengths\n",
    "    })\n",
    "\n",
    "    return labels_df, all_X_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_model = N_TRAIN_SAMPLES_PER_MODEL + N_TEST_SAMPLES_PER_MODEL\n",
    "labels_df, all_X_samples = generate_data(models_lst, samples_per_model, MAX_SAMPLE_LEN, MIN_SAMPLE_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_ids = []\n",
    "test_samples_ids = []\n",
    "for label, sub_df in labels_df.groupby(\"true_label\"):\n",
    "    train_ids = sub_df.sample(N_TRAIN_SAMPLES_PER_MODEL).index.values\n",
    "    test_ids = sub_df.drop(train_ids).index.values\n",
    "    if len(test_ids) != N_TEST_SAMPLES_PER_MODEL:\n",
    "        raise Exception(f\"len(test_ids) {len(test_ids)} != N_TEST_SAMPLES_PER_MODEL {N_TEST_SAMPLES_PER_MODEL}\")\n",
    "    train_samples_ids.extend(train_ids)\n",
    "    test_samples_ids.extend(test_ids)\n",
    "    \n",
    "\n",
    "indices_split = {\n",
    "    \"train_samples_ids\": train_samples_ids, \n",
    "    \"test_samples_ids\": test_samples_ids}\n",
    "\n",
    "indices_splits_lst = [indices_split]\n",
    "print(len(indices_split[\"train_samples_ids\"]), len(indices_split[\"test_samples_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = f\"{FILENAME_SUFFIX}_{time_index}.pkl\"\n",
    "output_path = f\"{DATA_DIR}/{data_filename}\"\n",
    "\n",
    "metadata = {\n",
    "    \"N_TRAIN_SAMPLES_PER_MODEL\": N_TRAIN_SAMPLES_PER_MODEL,\n",
    "    \"N_TEST_SAMPLES_PER_MODEL\": N_TEST_SAMPLES_PER_MODEL,\n",
    "    \"MAX_SAMPLE_LEN\": MAX_SAMPLE_LEN,\n",
    "    \"MIN_SAMPLE_LEN\": MIN_SAMPLE_LEN\n",
    "}\n",
    "\n",
    "data = {\n",
    "    'data_filename': data_filename,\n",
    "    'time_index': time_index,\n",
    "    'models_lst': models_lst,\n",
    "    'labels_df': labels_df,\n",
    "    'all_X_samples': all_X_samples,\n",
    "    'indices_splits_lst': indices_splits_lst,\n",
    "    'metadata': metadata\n",
    "}\n",
    "\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "    print(f\"Data saved to {output_path}.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "824dc94f4a21fb2b9b063d5374c6a5be71ea0704887dd3ffa4e3d703212d1775"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hmm_dtw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
