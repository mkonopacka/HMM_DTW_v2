{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dtw\n",
    "from statistics import mode\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "RESULTS_DIR = \"results_HMM\"\n",
    "DATA_PICKLE_FILE = \"data_HMM/HMM_data_ntrain25_1662023877.pkl\"\n",
    "SPLIT_ID = 0\n",
    "K_LST = [1,3,5,10,20,30,100] # list of used K, don't change\n",
    "PARTIAL_MATCHES = True   # if True, allow for partial matches\n",
    "CLS_NAME = \"DTW\"\n",
    "WINDOW_SIZE = None # \n",
    "window = \"\" if WINDOW_SIZE is None else f\"w{WINDOW_SIZE}_\"\n",
    "part = \"part_\" if PARTIAL_MATCHES else \"\"\n",
    "FILENAME_INFIX = f\"dtw_{part}{window}results_for\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['generating_model', 'data_filename', 'time_index', 'models_lst', 'labels_df', 'all_X_samples', 'all_Z_samples', 'indices_splits_lst', 'metadata', 'description'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(DATA_PICKLE_FILE, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N_TRAIN_SAMPLES_PER_MODEL': 25, 'N_TEST_SAMPLES_PER_MODEL': 100, 'MAX_SAMPLE_LEN': 128, 'MIN_SAMPLE_LEN': 90}\n"
     ]
    }
   ],
   "source": [
    "print(data[\"metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_splits_lst = data[\"indices_splits_lst\"]\n",
    "all_X_samples = data[\"all_X_samples\"]\n",
    "labels_df = data[\"labels_df\"]\n",
    "current_split = indices_splits_lst[SPLIT_ID]\n",
    "train_samples_ids = current_split[\"train_samples_ids\"]\n",
    "test_samples_ids = current_split[\"test_samples_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(current_split[\"test_samples_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify test samples with DTW\n",
    "For each test sample find DTW distance to all samples in train subset and classify as the closest one's label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try cross-validation to choose better k\n",
    "def classify_sample(X: np.ndarray, train_samples: list[np.ndarray], train_labels: list[int], k_lst: list[int],\n",
    "partial = PARTIAL_MATCHES, window = WINDOW_SIZE) -> dict[int, int]:\n",
    "    \"\"\"Classify a sample using DTW with k-NN based on samples in `train_samples` with different values of k.\n",
    "    Returns a mapping k -> returned label. \n",
    "    If `partial` is set to True, partial mapping is found by letting go of connected endpoints \n",
    "    contraint from classic DTW version. Uses default window type and step pattern from dtw library.\"\"\"\n",
    "    results = dict()\n",
    "    distances = []\n",
    "    for Y in train_samples:\n",
    "        dtw_obj = dtw.dtw(\n",
    "            X, Y, open_begin= partial, open_end = partial,\n",
    "            step_pattern = dtw.asymmetric if partial else dtw.symmetric2\n",
    "            )\n",
    "        normalized_dist = dtw_obj.normalizedDistance\n",
    "        distances.append(normalized_dist)\n",
    "    sorted_distances = np.argsort(distances)\n",
    "    for k in k_lst:\n",
    "        if k >= len(train_samples):\n",
    "            raise ValueError(\"k >= number of train examples doesn't make sense in KNN-classifier.\")\n",
    "        first_k = sorted_distances[:k]\n",
    "        results[k] = mode(train_labels[first_k])\n",
    "    return results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = labels_df.loc[train_samples_ids]\n",
    "train_ids = train_df.index.values.astype('int')\n",
    "train_samples = [all_X_samples[id] for id in train_ids]\n",
    "train_labels = train_df[\"true_label\"].values\n",
    "\n",
    "test_df = labels_df.loc[test_samples_ids]\n",
    "test_ids = test_df.index.values.astype('int')\n",
    "test_samples = [all_X_samples[id] for id in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying test samples:  55%|█████▌    | 499/900 [01:40<01:25,  4.67it/s]"
     ]
    }
   ],
   "source": [
    "# Remove too small k from k_lst based on train size\n",
    "k_lst = [k for k in K_LST if k <= len(train_samples)]\n",
    "start = time.time()\n",
    "k_to_label_mappings_lst = [\n",
    "    classify_sample(X, train_samples, train_labels, k_lst) \n",
    "    for X in tqdm(test_samples, desc = \"Classifying test samples\")\n",
    "    ]\n",
    "\n",
    "stop = time.time()\n",
    "total_time = stop - start\n",
    "print(f\"Time needed to classify all test samples: {total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict = {\n",
    "    k: tuple([d[k] for d in k_to_label_mappings_lst])\n",
    "    for k in k_lst\n",
    "}\n",
    "\n",
    "dfs_dict = {\n",
    "    k: pd.DataFrame({\"pred\": predictions_dict[k]}, index= test_df.index)\n",
    "    for k in k_lst\n",
    "}\n",
    "\n",
    "predictions_dfs = {\n",
    "    f\"{k}NN\": test_df.join(dfs_dict[k])\n",
    "    for k in k_lst\n",
    "}\n",
    "\n",
    "predictions_dfs[\"1NN\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many plots based on # used k values\n",
    "ncols = int(np.ceil(np.sqrt(len(k_lst))))\n",
    "nrows = int(np.ceil(len(k_lst)/ncols))\n",
    "\n",
    "y_true = test_df[\"true_label\"]\n",
    "plt.rcParams[\"figure.figsize\"] = (5*ncols, 5*nrows)\n",
    "plt.rcParams['font.size'] = min(5*nrows, 28)\n",
    "fig, axes = plt.subplots(ncols = ncols, nrows = nrows)\n",
    "\n",
    "for i,k in enumerate(k_lst):\n",
    "    max_i = i\n",
    "    ax = axes.flat[i]\n",
    "    y_pred = predictions_dfs[f\"{k}NN\"][\"pred\"]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='g', ax=ax, cbar = False)\n",
    "    ax.matshow(cm)\n",
    "    ax.set_title(f\"DTW with {k}NN\")\n",
    "\n",
    "# hide unused axes\n",
    "for i in range(max_i+1,len(axes.flat)):\n",
    "    axes.flat[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = dict()\n",
    "for variant, df in predictions_dfs.items():\n",
    "    if PARTIAL_MATCHES:\n",
    "        variant = f\"{variant}_partial\"\n",
    "    variant = variant + f\"{window}\"\n",
    "    acc = utils.accuracy(y_true, df[\"pred\"])\n",
    "    accuracies[variant] = acc\n",
    "    print(f\"{variant} accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = data[\"time_index\"]\n",
    "summary = {\n",
    "    \"data_filename\": DATA_PICKLE_FILE,\n",
    "    \"classificator\": CLS_NAME,\n",
    "    \"time_index\": time_index,\n",
    "    \"split_id\": SPLIT_ID,\n",
    "    \"predictions_dfs\": predictions_dfs,\n",
    "    \"accuracies\": accuracies,\n",
    "    \"total_time\": total_time\n",
    "}\n",
    "\n",
    "for key, val in summary.items():\n",
    "    print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_type = data[\"generating_model\"]\n",
    "split_id_str = str(SPLIT_ID)\n",
    "if len(split_id_str) == 1:\n",
    "    split_id_str = \"0\" + split_id_str\n",
    "\n",
    "output_path = f\"{RESULTS_DIR}/{FILENAME_INFIX}_{gen_type}_{time_index}_{split_id_str}.pkl\"\n",
    "if os.path.exists(output_path):\n",
    "    inp = input(f\"Classification results {output_path} already exists. Do you want to overwrite it? (y = yes)\")\n",
    "    if inp in [\"y\", \"Y\"]:\n",
    "        with open(output_path, 'wb') as f:\n",
    "                pickle.dump(summary, f)\n",
    "                print(f\"Updated classification results saved to {output_path}.\")\n",
    "    else:\n",
    "        print(\"Canceled.\")\n",
    "else:\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(summary, f)\n",
    "        print(f\"Classification results saved to {output_path}.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "824dc94f4a21fb2b9b063d5374c6a5be71ea0704887dd3ffa4e3d703212d1775"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hmm_dtw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
